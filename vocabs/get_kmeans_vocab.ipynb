{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, T5Tokenizer, T5EncoderModel, AutoModel\n",
    "import torch\n",
    "import json\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_n = 2000\n",
    "model_name = 'google/flan-t5-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_t5_embedding(word):\n",
    "    input_ids = tokenizer.encode(word, add_special_tokens=False, return_tensors='pt')\n",
    "    # print(input_ids)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.encoder.embed_tokens(input_ids)\n",
    "        # print(outputs)\n",
    "        pooled_embedding = torch.mean(outputs, dim=1).squeeze().numpy()\n",
    "    return pooled_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(word_dict):\n",
    "    embeddings = []\n",
    "    embed_dict = {}\n",
    "    for word in word_dict.keys():\n",
    "        embedding = get_t5_embedding(word)\n",
    "        embed_dict[word] = embedding\n",
    "        embeddings.append(embedding)\n",
    "    return np.array(embeddings), embed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_token(text: str, special_token: str) -> str:\n",
    "    return text.replace(special_token, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "vocab = t5_tokenizer.get_vocab()\n",
    "special_tokens = [t5_tokenizer.unk_token, t5_tokenizer.pad_token, t5_tokenizer.sep_token, t5_tokenizer.cls_token]\n",
    "special_space = '▁'\n",
    "vocab = {word: index for word, index in vocab.items() if word not in special_tokens and special_space in word}\n",
    "word_embeddings, embed_dict = generate_embeddings(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_key, vocab_value = list(vocab.keys()), list(vocab.values())\n",
    "print(len(vocab_key), len(vocab_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embed_dict to json\n",
    "new_embed_dict = {}\n",
    "for k, v in embed_dict.items():\n",
    "    new_embed_dict[k] = v.tolist()\n",
    "if 't5' in model_name:\n",
    "    model_name = model_name.replace('/', '-')\n",
    "    with open(str(final_n)+'-'+model_name+'-embed_dict.json', 'w') as fp:\n",
    "        json.dump(new_embed_dict, fp, indent=4, ensure_ascii=False)\n",
    "else:\n",
    "    with open(str(final_n)+'-'+model_name+'-embed_dict.json', 'w') as fp:\n",
    "            json.dump(new_embed_dict, fp, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_embed_dict['▁hello']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embed_dict['▁hello']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the clusters\n",
    "from sklearn.decomposition import PCA\n",
    "print(word_embeddings.shape)\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(word_embeddings)\n",
    "pca_embeddings = pca.transform(word_embeddings)\n",
    "pca_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement kmeans clustering\n",
    "from sklearn.cluster import KMeans\n",
    "# cluster on the word embeddings\n",
    "kmeans = KMeans(n_clusters=final_n, random_state=0).fit(word_embeddings)\n",
    "kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the centroid words\n",
    "centroid_words = []\n",
    "for centroid in kmeans.cluster_centers_:\n",
    "    index = np.argmin(np.linalg.norm(word_embeddings - centroid, axis=1))\n",
    "    print(index)\n",
    "    centroid_words.append(vocab_key[index])\n",
    "centroid_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the new centroid words as a new vocab\n",
    "new_vocab = {word: vocab[word] for index, word in enumerate(centroid_words)}\n",
    "with open(str(final_n)+'-'+model_name+'-kmeans-vocab.json', 'w') as fp:\n",
    "    json.dump(new_vocab, fp, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the clusters\n",
    "plt.scatter(pca_embeddings[:, 0], pca_embeddings[:, 1], c=kmeans.labels_, cmap='rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the clusters using t-sne\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, n_iter=1000, random_state=0)\n",
    "tsne_embeddings = tsne.fit_transform(word_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cluster\n",
    "plt.scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], c=kmeans.labels_, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings[6].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
